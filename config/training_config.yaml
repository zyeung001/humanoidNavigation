
standing:
  # ========== ENVIRONMENT CONFIGURATION ==========
  
  # Basic settings
  seed: 42
  n_envs: 8  # Parallel environments
  device: cuda  # 'cuda' or 'cpu'
  
  # Episode settings
  max_episode_steps: 10000  # Increased for indefinite standing training
  
  # ========== OBSERVATION PROCESSING ==========
  # Enhanced observations for better state estimation
  
  obs_history: 4  # Stack 4 frames for temporal information
  obs_include_com: true  # Include center-of-mass features
  obs_feature_norm: true  # Normalize features with tanh
  
  # ========== ACTION PREPROCESSING ==========
  
  action_smoothing: true
  action_smoothing_tau: 0.5
  
  action_symmetry: false  # Don't enforce left-right symmetry
  pd_assist: false  # No PD control assistance
  
  # ========== CURRICULUM LEARNING ==========
  curriculum_start_stage: 0
  curriculum_max_stage: 4  
  curriculum_advance_after: 20  # Episodes before considering advancement
  curriculum_success_rate: 0.60  # Success rate needed to advance
  
  
  # ========== DOMAIN RANDOMIZATION ==========
  
  domain_rand: true
  rand_mass_range: [0.95, 1.05]  
  rand_friction_range: [0.95, 1.05]  
  
  # ========== REWARD FUNCTION  ==========
  target_height: 1.40
  height_error_threshold: 0.08
  height_stability_threshold: 0.15
  

  reward_caps:
    max_height_maintenance_penalty: 15.0  
    recovery_bonus_scale: 50.0  
    termination_penalty_constant: 50.0  
  
  # ========== PPO HYPERPARAMETERS ==========
  
  # Training schedule
  total_timesteps: 10_000_000  # Increased for indefinite standing capability
  learning_rate: 0.0003
  final_learning_rate: 0.00005  # Linear decay
  
  # PPO algorithm parameters
  n_steps: 2048  # Steps per environment per update
  batch_size: 256  # Minibatch size
  n_epochs: 10  # Gradient steps per update
  
  gamma: 0.995  # Discount factor (long horizon for standing)
  gae_lambda: 0.95  # GAE parameter
  
  # Policy optimization
  clip_range: 0.2
  final_clip_range: 0.1  # Linear decay
  
  # Entropy regularization
  ent_coef: 0.02  # Initial exploration
  final_ent_coef: 0.005  # Final exploitation
  
  # Value function
  vf_coef: 0.5
  
  # Gradient clipping
  max_grad_norm: 0.5
  
  # ========== VECNORMALIZE CONFIGURATION ==========
  
  normalize: true
  vecnormalize_clip_obs: 50.0 
  vecnormalize_clip_reward: 50.0  
  
  
  # ========== NETWORK ARCHITECTURE ==========
  
  policy_kwargs:
    net_arch:
      - pi: [512, 512, 256]  # Policy network
        vf: [512, 512, 256]  # Value network
    activation_fn: SiLU  # Smooth activation (better than ReLU)
    ortho_init: true  # Orthogonal initialization
  
  # ========== LOGGING & CHECKPOINTING ==========
  
  verbose: 1
  
  log_dir: data/logs
  use_tensorboard: true
  
  # Evaluation
  n_eval_episodes: 5
  eval_freq: 100_000  # Every 100k steps
  
  # Checkpointing
  save_freq: 250_000  # Every 250k steps
  checkpoint_dir: data/checkpoints
  checkpoint_prefix: standing
  
  # Model saving
  best_model_path: models/best_standing_model
  final_model_path: models/final_standing_model
  vecnormalize_path: models/vecnorm.pkl
  
  # Success thresholds
  target_reward_threshold: 5000
  
  # Early stopping
  early_stop:
    patience: 5  # Number of consecutive successes needed
  
  # ========== ADVANCED OPTIONS ==========
  
  # Random height initialization for recovery training
  random_height_init: true  
  random_height_prob: 0.3  # 30% of episodes start at random height
  random_height_range: [-0.3, 0.1]  # Mostly downward perturbations
  
  # This forces the agent to experience and recover from low heights
  # even when curriculum is at high stages
  
  # WandB logging 
  use_wandb: false
  wandb_project: humanoid_standing
  wandb_log_freq: 10000
  video_freq: 500000  # Record video every 500k steps
  episode_print_freq: 1000


# ============================================================================
# WALKING CONFIGURATION
# ============================================================================

walking:
  # ========== ENVIRONMENT CONFIGURATION ==========
  
  # Basic settings
  seed: 42
  n_envs: 8  # Parallel environments
  device: cuda  # 'cuda' or 'cpu'
  
  # Episode settings
  max_episode_steps: 5000
  
  # ========== OBSERVATION PROCESSING ==========
  # Walking uses same obs processing as standing + velocity commands
  
  obs_history: 4  # Stack 4 frames for temporal information
  obs_include_com: true  # Include center-of-mass features (+6 dims)
  obs_feature_norm: true  # Normalize features with tanh
  # Note: Walking adds +2 dims for commanded velocity
  # Total: (365 + 6 + 2) * 4 = 1492 dims
  
  # ========== ACTION PREPROCESSING ==========
  
  action_smoothing: true
  action_smoothing_tau: 0.25  # Slightly higher for stability during recovery
  
  action_symmetry: false
  pd_assist: false
  
  # ========== WALKING-SPECIFIC ==========
  
  velocity_weight: 4.0  # Slightly reduced for stability during recovery
  # Velocity reward components (stabilized):
  # - velocity_match_reward: up to 8 points
  # - precision_bonus: up to 4 points  
  # - direction_reward: up to 6 points
  # - speed_match_bonus: up to 4 points
  # - movement_shaping: up to 3 points
  # Total positive velocity reward: up to ~25 points
  max_commanded_speed: 0.3  # Starting speed (curriculum controls this)
  
  # Speed curriculum stages (m/s) - progressive from very slow to running
  curriculum_max_speed_stages: [0.3, 0.6, 1.0, 1.5, 2.0, 2.5, 3.0]
  
  # ========== CURRICULUM LEARNING (RELAXED) ==========
  curriculum_start_stage: 0
  curriculum_max_stage: 4  # Target stage 4 = 2.0 m/s (light jog)
  # Stage 0: 0.3 m/s (very slow walk) - easiest
  # Stage 1: 0.6 m/s (slow walk)
  # Stage 2: 1.0 m/s (normal walk)
  # Stage 3: 1.5 m/s (fast walk)
  # Stage 4: 2.0 m/s (light jog)
  # Stage 5: 2.5 m/s (jog)
  # Stage 6: 3.0 m/s (run)
  curriculum_advance_after: 15  # Faster iteration
  curriculum_success_rate: 0.50  # RELAXED: 50% success rate to advance
  
  # Direction diversity - train with different velocity directions
  direction_diversity: true
  
  # ========== ROBUSTNESS TRAINING ==========
  # Push perturbations for recovery training
  push_enabled: true
  push_interval: 250  # Apply push every N steps
  push_magnitude_range: [30.0, 100.0]  # Force in Newtons (start gentle)
  push_duration: 4  # Steps to apply push
  
  # ========== DOMAIN RANDOMIZATION ==========
  
  domain_rand: false  # Start without, curriculum enables in later stages
  rand_mass_range: [0.97, 1.03]  # Gentler initially
  rand_friction_range: [0.97, 1.03]  
  
  # ========== REWARD FUNCTION ==========
  target_height: 1.40
  height_error_threshold: 0.15  # Slightly relaxed for walking
  height_stability_threshold: 0.20
  
  reward_caps:
    max_height_maintenance_penalty: 15.0  
    recovery_bonus_scale: 50.0  
    termination_penalty_constant: 50.0  
  
  # ========== PPO HYPERPARAMETERS ==========
  
  # Training schedule - more conservative for stability
  total_timesteps: 10_000_000  # Reduced - should converge faster with stable rewards
  learning_rate: 0.0001  # Lower LR for stability (was 0.0003)
  final_learning_rate: 0.00003  # Linear decay
  
  # PPO algorithm parameters
  n_steps: 2048  # Steps per environment per update
  batch_size: 256  # Minibatch size
  n_epochs: 10  # Gradient steps per update
  
  gamma: 0.995  # Discount factor
  gae_lambda: 0.95  # GAE parameter
  
  # Policy optimization
  clip_range: 0.2
  final_clip_range: 0.1  # Linear decay
  
  # Entropy regularization
  ent_coef: 0.02  # Initial exploration
  final_ent_coef: 0.005  # Final exploitation
  
  # Value function
  vf_coef: 0.5
  
  # Gradient clipping
  max_grad_norm: 0.5
  
  # ========== VECNORMALIZE CONFIGURATION ==========
  
  normalize: true
  vecnormalize_clip_obs: 50.0 
  vecnormalize_clip_reward: 50.0  
  
  # ========== NETWORK ARCHITECTURE ==========
  # Same architecture as standing - [512, 512, 256] is plenty
  
  policy_kwargs:
    net_arch:
      - pi: [512, 512, 256]  # Policy network
        vf: [512, 512, 256]  # Value network
    activation_fn: SiLU  # Smooth activation
    ortho_init: true  # Orthogonal initialization
  
  # ========== LOGGING & CHECKPOINTING ==========
  
  verbose: 1
  
  log_dir: data/logs/walking
  use_tensorboard: true
  
  # Evaluation
  n_eval_episodes: 5
  eval_freq: 100_000  # Every 100k steps
  
  # Checkpointing
  save_freq: 250_000  # Every 250k steps
  checkpoint_dir: data/checkpoints/walking
  checkpoint_prefix: walking
  
  # Model saving
  best_model_path: models/best_walking_model
  final_model_path: models/final_walking_model
  vecnormalize_path: models/vecnorm_walking.pkl
  
  # Success thresholds
  target_reward_threshold: 3000
  velocity_error_threshold: 0.4  # m/s - max acceptable velocity tracking error
  
  # Early stopping
  early_stop:
    patience: 5  # Number of consecutive successes needed
  
  # ========== ADVANCED OPTIONS ==========
  
  # Random height initialization for recovery training
  random_height_init: true  
  random_height_prob: 0.25  # 25% of episodes start with perturbation
  random_height_range: [-0.25, 0.1]  # Mostly downward perturbations to train recovery
  
  # Velocity perturbations at reset
  random_velocity_init: true
  random_velocity_range: [-0.3, 0.3]  # m/s - starting velocity perturbation
  
  # WandB logging 
  use_wandb: false
  wandb_project: humanoid_walking
  wandb_log_freq: 10000
  video_freq: 500000  # Record video every 500k steps
  episode_print_freq: 1000