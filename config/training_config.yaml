# In training_config.yaml, update hyperparameters to match successful MuJoCo PPO setups (e.g., larger clip_range for bolder updates, lower ent_coef to focus on exploitation, smaller n_steps for frequent updates, larger network for complex balance).
# Replace the standing block with:
standing:
  algorithm: "PPO"
  total_timesteps: 1000000  
  learning_rate: 0.0003

  batch_size: 512
  n_steps: 2048  # Smaller rollouts for more updates
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2  # Wider for better exploration
  ent_coef: 0.001  # Lower to reduce randomness in later training
  vf_coef: 0.5
  max_grad_norm: 0.5

  n_eval_episodes: 5
  target_height: 1.4  # Adjust to match initial torso height
  
  policy_kwargs:
    net_arch:
      pi: [512, 256, 128]  # Larger for high-dim control
      vf: [512, 256, 128]
    activation_fn: "relu"
    log_std_init: -0.5  
  
  use_wandb: true
  wandb_project: "humanoid-standing-improved"
  wandb_log_freq: 2000
  video_freq: 50000
  
  height_error_threshold: 0.05
  height_stability_threshold: 0.05
  target_reward_threshold: 800  # Keep, but expect ~10-20 per step for good standing
  
  normalize: true
  n_envs: 8
  
  video_n_episodes: 2
  video_max_steps: 500

  episode_print_freq: 1000

  best_model_path: "models/saved_models/best_standing_model"
  final_model_path: "models/saved_models/final_standing_model"
  vecnormalize_path: "models/saved_models/vecnorm.pkl"
  checkpoint_dir: "models/saved_models/checkpoints"
  checkpoint_prefix: "checkpoint"

  max_episode_steps: 2000

  domain_rand: true
  rand_mass_range: [0.85, 1.15]
  rand_friction_range: [0.8, 1.2]